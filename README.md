# ğŸ‘¨â€ğŸ’» Jiajun (Thomas) Zhou

#### Ph.D. Student @ HKU | Research Associate @ UCSB | Specializing in Efficient AI, LLM Optimization, and Hardware-Aware Training
- ğŸ“« zhoutomas177@gmail.com | [LinkedIn](https://linkedin.com/in/jiajun-thomas-c-38a22bb2)




## ğŸ“ Education

**The University of Hong Kong (HKU) â€” _Ph.D. in Electrical and Electronic Engineering (2022 - Present)**
- Focus: Efficient LLM training/inference, quantization, reasoning, and edge deployment  
- Supervised by Prof. Ngai Wong  
- Publications in ACL, NAACL, FCCM, TCAD

**Hong Kong University of Science and Technology (HKUST) â€” _M.S. in IC Design Engineering_(2018 â€“ 2019)**  
- GPA: 3.7  
- Coursework in VLSI Design, Embedded Systems, Semiconductor Devices

**National Huaqiao University â€” _B.Eng. in Integrated Circuit Design_(2014 â€“ 2018)**  
- Multiple scholarships and an exchange student experience at Taiwan


## ğŸ’¼ Experience

### **Research Associate @ UCSB**_Sept. 2023 â€“ Present | Advisor: Prof. Zheng Zhang_  
- Developed low-bit quantized fine-tuning techniques for LLMs (QuZO, LoRA variants)  
- Collaborated with Amazon AGI team on scalable training paradigms  
- **NAACL 2024 Spotlight Paper**: Demonstrated high scalability vs. other tuning baselines  

### **Research Assistant @ CUHK** _Apr. 2021 â€“ Dec. 2021 | Advisor: Prof. Guoliang Xing_  
- Co-designed FPGA-GPU hybrid acceleration schemes (40% better deadline adherence)  
- Led NFC wireless charging system project from concept to prototype  

### **Mixed-Signal IC Design Engineer @ ASTRI** _Sep. 2019 â€“ Mar. 2021 | Technology Co-Design Group_  
- Designed key analog IPs, including ADCs, comparators,and  amplifiers  
- Delivered taped-out chip with 10-bit ADC and PMU subsystems

---

## ğŸ§  Selected Publications

1. **QuZO: Quantized Zeroth-Order Fine-Tuning for LLMs** â€“ _ACL 2025 (Under Review)_  
2. **LoRETTA: Tensor-Train Adaptation for LLMs** â€“ _NAACL 2024 (Spotlight Paper)_  
3. **DyBit: Dynamic Bit-Precision Inference** â€“ _IEEE TCAD 2023_  
4. **MSD: Mixing Signed Digits on FPGAs** â€“ _FCCM 2023_  
5. **NoiseZO: RRAM-Driven ZO Optimization** â€“ _DAC 2025_  
6. **PECAN: Product-Quantized CAM Network** â€“ _DATE 2023_  
7. **Lite It Fly: All-Deformable Butterfly Network** â€“ _TNNLS (in brief)_  
8. **Reconfigurable PE for HPC** â€“ _IEEE TCS-II, 2023_  
9. **Lightweight CNNs from Tensor Perspective** â€“ _ASICON 2023_  
10. **Memristor-Based CNN Acceleration** â€“ _ASICON 2023_

ğŸ“ Full publication list on [Google Scholar](#) _(add link if available)_

---

## ğŸš€ Research Highlights
**Efficient LLM Fine-Tuning**: 
Developed QuZO and LoRETTA frameworks to push the limit of parameter-efficient and quantized tuning strategies.
**Hardware-Aware ML**: 
Designed acceleration methods on FPGAs and analog-mixed chips for DNN inference and edge AI.
**Cross-Layer Co-Design**: 
Collaborated on vertical-stack optimization across algorithms, model architectures, and hardware implementation.


## ğŸ“· Fun Fact

I enjoy exploring the intersection of **AI algorithms and hardware**â€”whether it's crafting efficient transformer models, squeezing memory on an edge chip, or analyzing training curves in VS Code.

#### ğŸ”§ Technical Skills
**Languages:** Python, C/C++, MATLAB, Verilog  
**Frameworks & Platforms:** PyTorch, TensorFlow (incl. Lite & Keras), OpenMP, CUDA, Caffe  
**Tools:** Cadence, Xilinx Vivado & ISE, HSpice, Modelsim, VS Code  
