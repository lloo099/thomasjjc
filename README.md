# 📰 AI Researcher 
#### Ph.D. Student @ HKU | Research Associate @ UCSB | Specializing in Efficient AI, LLM Optimization, and Hardware-Aware Training
🚀 *Actively seeking full-time opportunities in AI research, ML systems, or hardware-software co-design starting in 2025/2026.*

 📫 zhoutomas177@gmail.com(Personal) | [LinkedIn](https://www.linkedin.com/in/jiajun-z-38a22bb2/)

 ✉️ ryjjc@connect.hku.hk  | 📍 Mountain View, California (Now)

---


## 🎓 Education

**Ph.D., EEE | The University of Hong Kong (December 2025)**
- Focus: Efficient LLM training/inference, quantization, reasoning, and edge deployment  
- Supervised by Prof. Ngai Wong  
- Publications in NLP/FPGA-related conferences/Journal

**M.S., IC Design | HKUST (December 2019)**  
- Advisor: Prof. Mansun
- Focus: VLSI Design, Embedded Systems, Semiconductor Devices

**B.Eng., IC Design | National Huaqiao University (June 2018)**  
- Multiple scholarships and an exchange student experience in Taiwan

---


## 💼 Experience

### **Research Intern @ Samsung Research America**
_May. 2025 – Present_  
- On-device LLM framework

### **Research Associate @ UCSB**
_Sept. 2023 – Apr. 2025 | Advisor: Prof. Zheng Zhang_  
- Developed low-bit quantized fine-tuning techniques for LLMs (QuZO, LoRA variants)  
- Collaborated with the Amazon AGI team on scalable training paradigms  
- **NAACL 2024 Spotlight Paper**: Demonstrated high scalability vs. other tuning baselines  

### **Research Assistant @ CUHK** 
_Apr. 2021 – Dec. 2021 | Advisor: Prof. Guoliang Xing_  
- Co-designed FPGA-GPU hybrid acceleration schemes 
- Led NFC wireless charging system project from concept to prototype  

### **Mixed-Signal IC Design Engineer @ ASTRI** 
_Sep. 2019 – Mar. 2021 | Technology Co-Design Group_  
- Designed key analog IPs, including ADCs, comparators, and  amplifiers  
- Delivered taped-out chip with 10-bit ADC and PMU subsystems

---

## 🧠 Selected Publications

1. **QuZO: Quantized Zeroth-Order Fine-Tuning for LLMs** – _(Under Review)_  
2. **LoRETTA: Tensor-Train Adaptation for LLMs** – _NAACL 2024 _  
3. **DyBit: Dynamic Bit-Precision Inference** – _IEEE TCAD 2023_  
4. **MSD: Mixing Signed Digits on FPGAs** – _FCCM 2023_  
5. **NoiseZO: RRAM-Driven ZO Optimization** – _DAC 2025_  
6. **PECAN: Product-Quantized CAM Network** – _DATE 2023_  
7. **Lite It Fly: All-Deformable Butterfly Network** – _TNNLS (in brief)_  


📝 Full publication list on [Google Scholar](https://scholar.google.com/citations?hl=en&user=4KQ6SKUAAAAJ)

---
## 🚀 Research Highlights
Machine learning and systems, with a focus on efficient training and inference:
- **Efficient LLM Fine-Tuning**: Developed QuZO and LoRETTA frameworks to push the limit of parameter-efficient and quantized tuning strategies. 
- **Hardware-Aware ML**: Designed acceleration methods on FPGAs and NPU chips for DNN inference and edge AI.
- **Algorithm/Hardware Co-Design**: Collaborated on the hardware compiler optimization across algorithms and model simulator.

---


## 📷 Fun Fact

I enjoy exploring the intersection of **AI algorithms and hardware**—whether it's crafting efficient LLM models, squeezing memory on an edge chip, or analyzing training efficiency.

---

## 🤝 Academic 

I'm passionate about bridging academia and decentralized technology—whether it’s co-authoring papers on efficient LLM training, collaborating with global research labs, or exploring blockchain infrastructure projects that bring AI infrastructure and intelligent agents on-chain.

---

#### 🔧 Technical Skills
**Languages:** Python, C/C++, MATLAB, Verilog  
**Frameworks & Platforms:** PyTorch, TensorFlow (incl. Lite & Keras), CUDA  
**Tools:** Cadence, Xilinx Vivado & ISE, HSpice, Modelsim, VS Code  

![Visitor Map](https://clustrmaps.com/map_v2.png?cl=080808&w=300&t=n&d=aGxQ8A_U40NaxF2z4ZaTII6flntLGJ5FUdm1qVRj-9g&co=ffffff&ct=808080)
